# -*- coding: utf-8 -*-
"""Street House View Numbers Image classification neural network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lXA1QpXXEap440w4k7ly1T4kkkaPVRMC

In this hands-on project the goal is to build a python code for image classification from scratch to understand the nitty gritties of building and training a model and further to understand the advantages of neural networks. First we will implement a simple KNN classifier and later implement a Neural Network to classify the images in the SVHN dataset. We will compare the computational efficiency and accuracy between the traditional methods and neural networks.

The Street View House Numbers (SVHN) Dataset: 

SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data formatting but comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.
Overview

The goal of this project is to take an image from the SVHN dataset and determine what that digit is. This is a multi-class classification problem with 10 classes, one for each digit 0-9. Digit '1' has label 1, '9' has label 9 and '0' has label 10.
Although, there are close to 6,00,000 images in this dataset, we have extracted 60,000 images
(42000 training and 18000 test images) to do this project. The data comes in a MNIST-like format of 32-by-32 RGB images centred around a single digit (many of the images do contain some distractors at the sides).
Reference

1. Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)

The SVHN data set is a multi class classification problem. The data are pictures of 32X32 pixels and the labels are rom 0 to 9.

For image classification it is required that the image's information is suitable converted into the algorithm's required format that will be implemented to build a model. Like performing array transition, converting labels into one hot encoders and normalizing the training data.
"""

import tensorflow as tf
tf.random.set_seed(42)

tf.__version__

#!pip install -U tensorflow==2.0

from google.colab import drive

drive.mount('/content/drive')

# As data is in the h1py format
import h5py

"""2. Data fetching and understand the train/val/test splits."""

# reading the h1py file
svhn = h5py.File('/content/drive/My Drive/Great Learning Residency 7.1 Neural Networks contd/project 2 neural networks/SVHN_single_grey1.h5', 'r')

# h5py.File acts like a Python dictionary, thus we can check the keys
 list(svhn.keys())

# respective values for each key in the h1py file
list(svhn.values())

# Shape of X_train depicts 18,000 images each of which is of 32X32 pixels So further down the line we will reshape it into 32*32=1024

fetch = list(svhn.keys())

# Get the data
X_test = list(svhn[fetch[0]])
X_train= list(svhn[fetch[1]])
X_val=list(svhn[fetch[2]])
y_test=list(svhn[fetch[3]])
y_train=list(svhn[fetch[4]])
y_val=list(svhn[fetch[5]])

print("length of X_train is :",len(X_train),"and","length of y_train is :",len(y_train))

print("length of X_test is :",len(X_test),"and","length of y_test is :",len(y_test))

print("length of X_val is :",len(X_val),"and","length of y_val is :",len(y_val))

"""On excluding the validation data set, 42000 is the train data and 18000 is the test data. There is a 70-30 train test split if the data size is considered to be 60000

Converting the data which is in list form to array form
"""

from numpy import array

x_train= array(X_train)
x_test=array(X_test)
y_train=array(y_train)
y_test=array(y_test)

x_train[0]

y_train

"""Visualizing the data"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10,10))
for i in range(10):
    plt.subplot(1,10,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i],cmap='gist_gray')  # cmap has various others options like cmaps['Sequential (2)'] = ['binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink','spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia','hot', 'afmhot', 'gist_heat', 'copper']
    
plt.show()
a=[]
for i in range(10):
    a.append(y_train[i])  

print ('label for each of the above image:',str(a)[1:-1])

"""Converting train and test labels to one hot vectors

We are given that SVHN is a multi-class classification problem. The dataset comprises of 10 classes represented with labels 0 to 9
"""

trainY = tf.keras.utils.to_categorical(y_train, num_classes=10) # that's why num classes=10 because we know 0 to 9 are he given class labels
testY = tf.keras.utils.to_categorical(y_test, num_classes=10)

trainY.shape

testY.shape

"""Normalizing both the train and test image data from 0-255 to 0-1"""

x_train/= 255
x_test/= 255

x_train[0]

x_train.shape

"""
Reshape the train and test data so that KNN can be performed over it.
The feature image is in 2 dimenstional format.
The feature should be converted to 1 dimentional format to predict through algorithmns.
Reshaping the 2 dimentional image data to one dimentional array."""

x_train = x_train.reshape(x_train.shape[0], 1024)   # x_train.shape[0] keeps the 42000 train data and 1024 because of reshape 32X 32 pixels
x_test = x_test.reshape(x_test.shape[0], 1024)

"""3. Implement and apply an optimal k-Nearest Neighbor (kNN) classifier (7.5 points)"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import numpy as np
import pandas as pd

"""Performing cross validation to find the optimal number of neighbours using the Mis classification error. Misclassification error (MSE) = 1 - Test accuracy score"""

k=np.arange(1,30,2)

k

accuracy_scores=[]
for i in k:
# Instantiate the model . 
  knn = KNeighborsClassifier(n_neighbors=i,algorithm='brute')
# Fit the model on the training data.
  knn=knn.fit(x_train, y_train)
# Predict the response for test dataset 
  y_pred = knn.predict(x_test)
# Storing the results 
  accuracy_scores.append(metrics.accuracy_score(y_test,y_pred))

accuracy_scores

# Calculating  Misclassification error (MSE) 
mse=[]
for i in accuracy_scores:
    mse.append(1-i)

mse

min(mse) # least mse

mse.index(0.46772222222222226)

k[13]

"""Plotting misclassification error vs k (with k value on X-axis) using matplotlib."""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
plt.plot(k,mse)

knn_model = KNeighborsClassifier(n_neighbors=27,algorithm='brute')
knn_model = knn_model.fit(x_train, y_train)
y_pred_final = knn_model.predict(x_test)

# Getting the accuracy score for train data
knn_model.score(x_train, y_train)

# Getting the accuracy score for test data
knn_model.score(x_test, y_test)*100

# Getting accuracy for validation data

x_val=array(X_val)
y_val=array(y_val)

x_val/= 255

x_val.shape

x_val = x_val.reshape(x_val.shape[0], 1024)

knn_model.score(x_val, y_val)

"""4. Print the classification metric report (2.5 points)"""

print(metrics.classification_report(y_test, y_pred_final))
#Precision â€“ Accuracy of positive predictions
#Recall - Fraction of positives That were correctly identified.
#F1 Score - harmonic mean of precision and recall

"""With k=27,  the test accuray as 53.22 was achieved.

5. Implement and apply a deep neural network classifier including (feedforward neural network, RELU activations) (5 points)
"""

#Clear out tensorflow memory
tf.keras.backend.clear_session()

#Initialize Sequential model
model = tf.keras.models.Sequential()

#Add 1st hidden layer
model.add(tf.keras.layers.Dense(200, activation='relu'))

#Add 2nd hidden layer
model.add(tf.keras.layers.Dense(100, activation='relu'))

#Add OUTPUT layer
model.add(tf.keras.layers.Dense(10, activation='softmax'))

#Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

model.fit(x_train,trainY,          
          validation_data=(x_test,testY),
          epochs=50)

"""Accuracy achieved is very low. Implementing backpropagation techniques

6. Understand and be able to implement (vectorized) backpropagation (cost stochastic gradient descent, cross entropy loss, cost functions) (2.5 points)
"""

#Clear out tensorflow memory
tf.keras.backend.clear_session()

#Initialize Sequential model
model1 = tf.keras.models.Sequential()

#Add 1st hidden layer
model1.add(tf.keras.layers.Dense(200, activation='relu'))

#Add 2nd hidden layer
model1.add(tf.keras.layers.Dense(100, activation='relu'))

#Add OUTPUT layer
model1.add(tf.keras.layers.Dense(10, activation='softmax'))

#Create optimizer with non-default learning rate
sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)

#Compile the model
model1.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model1.fit(x_train,trainY,          
          validation_data=(x_test,testY),
          epochs=50,
          batch_size=20)

"""With SGD optimizer with learning rate 0.03 and categorical cross entropy function , the accuray of the model has increased significantly.
Implementing batch hnormalization.

7. Implement batch normalization for training the neural network (2.5 points)
"""

#Clear out tensorflow memory
tf.keras.backend.clear_session()

#Initialize Sequential model
model2 = tf.keras.models.Sequential()

#Normalize the data
model2.add(tf.keras.layers.BatchNormalization())

#Add 1st hidden layer
model2.add(tf.keras.layers.Dense(200, activation='relu'))

#Add 2nd hidden layer
model2.add(tf.keras.layers.Dense(100, activation='relu'))

#Add OUTPUT layer
model2.add(tf.keras.layers.Dense(10, activation='softmax'))

#Create optimizer with non-default learning rate
sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)

#Compile the model
model2.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

model2.fit(x_train,trainY,          
          validation_data=(x_test,testY),
          epochs=50,
          batch_size=100)

# Evaluating for test data
model2.evaluate(x_test,testY)

# Evaluating for train data
model2.evaluate(x_train,trainY)

valY = tf.keras.utils.to_categorical(y_val, num_classes=10)

# Evaluating for validation data
model2.evaluate(x_val,valY)

predictionsNN = model2.predict(x_test)

ClassificationReportNN = metrics.classification_report(testY.argmax(axis=1), predictionsNN.argmax(axis=1))

print(ClassificationReportNN)

"""8. Understand the differences and trade-offs between traditional and NN classifiers with the help of classification metrics (5 points)

From the above values of KNN and Neural Network, it is evident there is a very bigchange in accuracy and classification metrics. KNN : 53.22% accurate whereas DNN: 84.34% accurate

The classification metrics for test data for both the models: for KNN at an average : precision : 54%, Recall : 53% whereas for DNN at an average: precision : 84%, recall : 84%
"""

# Computing classsification metrics for both the KNN model and Neural Network model (model2) for the validation data.

"""1) KNN classification report for validation data"""

y_pred_val = knn_model.predict(x_val)

print(metrics.classification_report(y_val, y_pred_val))

"""2) Neural Network classification report for validation data"""

predictionsNN_val = model2.predict(x_val)

ClassificationReportNN_val = metrics.classification_report(valY.argmax(axis=1), predictionsNN_val.argmax(axis=1))

print(ClassificationReportNN_val)

"""From the above values of KNN and Neural Network, it is evident there is a very bigchange in accuracy and classification metrics. KNN : 58.21% accurate whereas DNN: 91.16% accurate

The classification metrics for validation data for both the models: for KNN at an average : precision : 60%, Recall : 58% whereas for DNN at an average: precision : 91%, recall : 91%

Hence It can be concluded, the DNN works much better than traditional knn in the real time data like SVHN

INSIGHTS
1) The computational time for KNN is very much higher than the Deep Neural Network while predicting.

2) DNN yields better results with more data. As evident in classification report, there is utmost equally distributed data in all the classes. So DNN learns the each class better and yields better results.

3) K-NN accuracy depends on the K value. For any given training set, the best choice for K would be the one striking the optimal trade-off between bias and variance.
"""