# -*- coding: utf-8 -*-
"""Ensemble+Techniques_R3_Project1_Parkinson80%99s+Disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_QVUnUgFLFEINcnW3J5e072SOSilYRgH
"""

import numpy as np
import pandas as pd

"""Title: Parkinsons Disease Data Set

Abstract: Oxford Parkinson's Disease Detection Dataset

-----------------------------------------------------	

Data Set Characteristics: Multivariate
Number of Instances: 197
Area: Life
Attribute Characteristics: Real
Number of Attributes: 23
Date Donated: 2008-06-26
Associated Tasks: Classification
Missing Values? N/A

-----------------------------------------------------	

Source:

The dataset was created by Max Little of the University of Oxford, in 
collaboration with the National Centre for Voice and Speech, Denver, 
Colorado, who recorded the speech signals. The original study published the 
feature extraction methods for general voice disorders.

-----------------------------------------------------

Data Set Information:

This dataset is composed of a range of biomedical voice measurements from 
31 people, 23 with Parkinson's disease (PD). Each column in the table is a 
particular voice measure, and each row corresponds one of 195 voice 
recording from these individuals ("name" column). The main aim of the data 
is to discriminate healthy people from those with PD, according to "status" 
column which is set to 0 for healthy and 1 for PD.

The data is in ASCII CSV format. The rows of the CSV file contain an 
instance corresponding to one voice recording. There are around six 
recordings per patient, the name of the patient is identified in the first 
column.For further information or to pass on comments, please contact Max 
Little (littlem '@' robots.ox.ac.uk).

Further details are contained in the following reference -- if you use this 
dataset, please cite:
Max A. Little, Patrick E. McSharry, Eric J. Hunter, Lorraine O. Ramig (2008), 
'Suitability of dysphonia measurements for telemonitoring of Parkinson's disease', 
IEEE Transactions on Biomedical Engineering (to appear).

-----------------------------------------------------

Attribute Information:

Matrix column entries (attributes):
1) name - ASCII subject name and recording number
2) MDVP:Fo(Hz) - Average vocal fundamental frequency
3) MDVP:Fhi(Hz) - Maximum vocal fundamental frequency
4) MDVP:Flo(Hz) - Minimum vocal fundamental frequency
5) MDVP:Jitter(%),(6) MDVP:Jitter(Abs), (7) MDVP:RAP, (8) MDVP:PPQ, (9) Jitter:DDP - Several measures of variation in fundamental frequency
10) MDVP:Shimmer, (11) MDVP:Shimmer(dB), 12)Shimmer:APQ3, 13)Shimmer:APQ5, 14)MDVP:APQ, 15)Shimmer:DDA - Several measures of variation in amplitude
16)NHR, 17)HNR - Two measures of ratio of noise to tonal components in the voice

18)RPDE, 19)D2 - Two nonlinear dynamical complexity measures
20)DFA - Signal fractal scaling exponent
21)spread1,(22)spread2,(23)PPE - Three nonlinear measures of fundamental frequency variation 
24) status - Health status of the subject (one) - Parkinson's, (zero) - healthy

-----------------------------------------------------

Citation Request:

If you use this dataset, please cite the following paper: 
'Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection', 
Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. 
BioMedical Engineering OnLine 2007, 6:23 (26 June 2007)

Steps to be followed:
1. Load the dataset
"""

parkinsons=pd.read_csv('parkinsons.data')

parkinsons.head(5)

"""2. It is always a good practice to eye-ball raw data to get a feel of the data in terms of number of structure of the file, number of attributes, types of attributes and a general idea of likely challenges in the dataset. (2.5 points)"""

# structure
parkinsons.shape

#the number of attributes
len(parkinsons.columns)

# types of attributes
parkinsons.info()

parkinsons.isnull().values.any() # Checking for Null values

parkinsons.describe().transpose()

"""3. Using univariate and bivariate analysis to check the individual attributes for their basic statistic such as central          values, spread, tails etc. What are your observations? (15 points)"""

#Univariate analysis

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:Fo(Hz)']);

plt.subplot(122)
parkinsons['MDVP:Fo(Hz)'].plot.box(figsize=(16,5))

plt.show()

#The Average vocal fundamental frequency (MDVP:Fo(Hz)) for most instances lies in the range of 120 to 180 hz

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:Fhi(Hz)']);

plt.subplot(122)
parkinsons['MDVP:Fhi(Hz)'].plot.box(figsize=(16,5))

plt.show()

#The Maximum vocal fundamental frequency (MDVP:Fhi(Hz)) of most instances lies in the range 130 to 230 with quite some outliers having over 400 hz upto 650 hz of Maximum vocal fundamental frequency

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:Flo(Hz)']);

plt.subplot(122)
parkinsons['MDVP:Flo(Hz)'].plot.box(figsize=(16,5))

plt.show()

#The Minimum vocal fundamental frequency (MDVP:Flo(Hz)) of most instances lies in the range 80 to 180 with quite some outliers having over 225 hz of Minimum vocal fundamental frequency

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:Jitter(%)']);

plt.subplot(122)
parkinsons['MDVP:Jitter(%)'].plot.box(figsize=(16,5))

plt.show()

#The MDVP:Jitter(%)values for most of the instances lies in the range 0.002 to 0.008 with some outliers from 0.013 upto 0.037 (%)

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:Jitter(Abs)']);

plt.subplot(122)
parkinsons['MDVP:Jitter(Abs)'].plot.box(figsize=(16,5))

plt.show()

# The MDVP:Jitter(Abs) values for most of the instances lies in the range 0.00003 to 0.00007 with some outliers upto 0.00028

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:RAP']);

plt.subplot(122)
parkinsons['MDVP:RAP'].plot.box(figsize=(16,5))

plt.show()

# The MDVP:RAP values values for most of the instances lies in the range 0.001  to 0.005 with some outliers fro, 0.008 upto 0.025

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:PPQ']);

plt.subplot(122)
parkinsons['MDVP:PPQ'].plot.box(figsize=(16,5))

plt.show()

# The MDVP:PPQ values for most instances lies in the range  0.0020 to 0.0040 with some outliers upto 0.020

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['Jitter:DDP']);

plt.subplot(122)
parkinsons['Jitter:DDP'].plot.box(figsize=(16,5))

plt.show()

# The Jitter:DDP values for most instances lies in the range 0.005 to 0.015 with high amount of outliers from 0.02 upto 0.07

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:Shimmer']);

plt.subplot(122)
parkinsons['MDVP:Shimmer'].plot.box(figsize=(16,5))

plt.show()

# The MDVP:Shimmer values for most instances lies in the range 0.005 to 0.05 with ouliers having values above 0.07 upto 0.13

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:Shimmer(dB)']);

plt.subplot(122)
parkinsons['MDVP:Shimmer(dB)'].plot.box(figsize=(16,5))

plt.show()

# The MDVP:Shimmer(dB) values for most instances lies in the range 0.05 to 0.5 with ouliers having values above 0.7 upto 1.4

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['Shimmer:APQ3']);

plt.subplot(122)
parkinsons['Shimmer:APQ3'].plot.box(figsize=(16,5))

plt.show()

# the Shimmer:APQ3 values for most instances lies in the range 0.005 to 0.025 with ouliers having values above 0.04 upto 0.07

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['Shimmer:APQ5']);

plt.subplot(122)
parkinsons['Shimmer:APQ5'].plot.box(figsize=(16,5))

plt.show()

# The Shimmer:APQ5 values for most instances lies in the range 0.005 to 0.025 with ouliers having values above 0.04 upto 0.09

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['MDVP:APQ']);

plt.subplot(122)
parkinsons['MDVP:APQ'].plot.box(figsize=(16,5))

plt.show()

#The MDVP:APQ values for most instances lies in the range 0.005 to 0.03 with ouliers having values above 0.05 upto 0.14

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['Shimmer:DDA']);

plt.subplot(122)
parkinsons['Shimmer:DDA'].plot.box(figsize=(16,5))

plt.show()

# The Shimmer:DDA values for most instances lies in the range 0.025 to 0.06 with ouliers having values above 0.12 upto 0.20

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['NHR']);

plt.subplot(122)
parkinsons['NHR'].plot.box(figsize=(16,5))

plt.show()

# The NHR values for most instances lies in the range 0.01 to 0.03 with ouliers having values above 0.06 upto 0.30

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['HNR']);

plt.subplot(122)
parkinsons['HNR'].plot.box(figsize=(16,5))

plt.show()

# The HNR values for most instances lies in the range 18 to 26 with  a few  ouliers having values below 10

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['RPDE']);

plt.subplot(122)
parkinsons['RPDE'].plot.box(figsize=(16,5))

plt.show()

# The RPDE values for most instances lies in the range 0.35 to 0.6 with no outliers.

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['D2']);

plt.subplot(122)
parkinsons['D2'].plot.box(figsize=(16,5))

plt.show()

#The D2 values for most instances lies in the range 2 to 3 with  outliers over 3.5.

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['DFA']);

plt.subplot(122)
parkinsons['DFA'].plot.box(figsize=(16,5))

plt.show()

# The DFA values for most instances lies in the range 0.625 to 0.775 with no outliers.

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['spread1']);

plt.subplot(122)
parkinsons['spread1'].plot.box(figsize=(16,5))

plt.show()

#The spread1 values for most instances lies in the range -6.5 to -4.5 with  outliers over -3 upto -1

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['spread2']);

plt.subplot(122)
parkinsons['spread2'].plot.box(figsize=(16,5))

plt.show()

# The spread 2 values for most instances lies in the range 0.17 to 0.27 with outliers over 0.4 and below 0.0

plt.figure(1)
plt.subplot(121)
sns.distplot(parkinsons['PPE']);

plt.subplot(122)
parkinsons['PPE'].plot.box(figsize=(16,5))

plt.show()

#The PPE values for most instances lies in the range 0.15 to 0.27 with outliers over 0.4 upto 0.6

# the target variable
parkinsons['status'].value_counts().plot.bar()

parkinsons['status'].value_counts()

# Out of the total instances 147 are detected with parkinsons disease and 48 are healthy.

# Bivariate Analysis

import seaborn as sns

sns.boxplot(x='status',y='MDVP:Fo(Hz)',data=parkinsons)
plt.show()

# Peope with Average vocal fundamental frequency in the range 120 to 170 hz have pd  while those having above 175 are healthy

sns.boxplot(x='status',y='MDVP:Fhi(Hz)',data=parkinsons)
plt.show()

# Peope with Maximum vocal fundamental frequency in the range 120 to 200 hz tend to have pd while those having above 200 are healthy

sns.boxplot(x='status',y='MDVP:Flo(Hz)',data=parkinsons)
plt.show()

# Peope with Minimum vocal fundamental frequency in the range 85 to 130 hz tend to have pd while those having above 140 are healthy

sns.boxplot(x='status',y='MDVP:Jitter(%)',data=parkinsons)
plt.show()

#Peope with MDVP:Jitter(%) values  in the range 0.004 to 0.008 tend to have pd while those having less than 0.003  are healthy

sns.boxplot(x='status',y='MDVP:Jitter(Abs)',data=parkinsons)
plt.show()

# Peope with MMDVP:Jitter(Abs) values  in the range 0.00003 to 0.00006 tend to have pd while those having less than 0.00002  are healthy with some outliers

sns.boxplot(x='status',y='MDVP:RAP',data=parkinsons)
plt.show()

# Peope with MDVP:RAP values  in the range 0.002 to 0.004 tend to have pd while those having less than 0.002  are healthy with some outliers

sns.boxplot(x='status',y='MDVP:PPQ',data=parkinsons)
plt.show()

# Peope with MDVP:PPQ  values  in the range 0.0023 to 0.0048 tend to have pd while those having less than 0.0020  are healthy with some outliers

sns.boxplot(x='status',y='Jitter:DDP',data=parkinsons)
plt.show()

# Peope with Jitter:DDP  values  in the range 0.005 to 0.014 tend to have pd while those having less than 0.004  are healthy with some outliers

sns.boxplot(x='status',y='MDVP:Shimmer',data=parkinsons)
plt.show()

# Peope with MDVP:Shimmer  values  in the range 0.02 to 0.045 tend to have pd while those having less than 0.02  are healthy with some outliers

sns.boxplot(x='status',y='MDVP:Shimmer(dB)',data=parkinsons)
plt.show()

# Peope with MDVP:Shimmer(dB) values  in the range 0.2 to 0.4 tend to have pd while those having less than 0.2  are healthy with some outliers

sns.boxplot(x='status',y='Shimmer:APQ3',data=parkinsons)
plt.show()

# Peope with Shimmer:APQ3 values  in the range 0.01 to 0.025 tend to have pd while those having less than 0.005  are healthy with some outliers

sns.boxplot(x='status',y='Shimmer:APQ5',data=parkinsons)
plt.show()

# Peope with Shimmer:APQ5 values  in the range 0.01 to 0.025 tend to have pd while those having less than 0.005  are healthy with some outliers

sns.boxplot(x='status',y='MDVP:APQ',data=parkinsons)
plt.show()

# Peope with MDVP:APQ values  in the range 0.01 to 0.035 tend to have pd while those having less than 0.01  are healthy with some outliers

sns.boxplot(x='status',y='Shimmer:DDA',data=parkinsons)
plt.show()

# Peope with Shimmer:DDA  values in the range 0.025 to 0.065 tend to have pd while those having less than 0.025  are healthy with some outliers

sns.boxplot(x='status',y='NHR',data=parkinsons)
plt.show()

# Peope with NHR values  in the range 0.02 to 0.04 tend to have pd while those having less than 0.02 are healthy with some outliers

sns.boxplot(x='status',y='HNR',data=parkinsons)
plt.show()

# Peope with HNR values in the range 18 to 24 tend to have pd while those having more than 25 are healthy with some outliers

sns.boxplot(x='status',y='RPDE',data=parkinsons)
plt.show()

# Peope with RPDE values in the range 0.45 to 0.6 tend to have pd while those having less than 0.45  are healthy

sns.boxplot(x='status',y='DFA',data=parkinsons)
plt.show()

# Peope with DFA values in the range 0.655 to 0.755 tend to have pd while those having less than 0.655 are healthy

sns.boxplot(x='status',y='MDVP:Fo(Hz)',data=parkinsons)
plt.show()

# Peope with MDVP:Fo(Hz) values in the range 123 to 170 tend to have pd while those having more than 175  are healthy

sns.boxplot(x='status',y='spread1',data=parkinsons)
plt.show()

# Peope with spread 1 values in the range -6 to -4.5 tend to have pd with some outliers while those having less than -6 are healthy

sns.boxplot(x='status',y='spread2',data=parkinsons)
plt.show()

# Peope with spread 2 values in the range 0.2 to 0.3 tend to have pd  while those having less than 0.2 are healthy

sns.boxplot(x='status',y='D2',data=parkinsons)
plt.show()

# Peope with D2 values in the range 2.25 to 2.75 tend to have pd with some outliers while those having less than 2.4 are healthy with some outliers

sns.boxplot(x='status',y='PPE',data=parkinsons)
plt.show()

# Peope with PPE values in the range 0.17 to 0.27 tend to have pd with some outliers while those having less than 0.15 are healthy with some outliers

# multivariate ananlysis

sns.pairplot(parkinsons, hue = 'status')

parkinsons.corr()

#It is evident  that columns MDVP:Fo(Hz) , MDVP:Fhi(Hz) , MDVP:Flo(Hz) and hnr having negative correlation with status while spread1, spread2 and PPE are positively and more correlated with our target column status, when compared to the other columns.

"""4. Split the dataset into training and test set in the ratio of 70:30 (Training:Test)."""

X=parkinsons.drop('status', axis=1)
y=parkinsons['status']

X=X.drop('name',axis=1)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

"""5. Create the model using “entropy” method of reducing the entropy and fit it to training data. (5 points)"""

from sklearn.tree import DecisionTreeClassifier
dm=DecisionTreeClassifier(criterion = 'entropy')

dm.fit(X_train, y_train)

"""6. Test the model on test data and what is the accuracy achieved. Capture the predicted values and do a crosstab. (7.5 points)"""

y_pred= dm.predict(X_test)

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

print(metrics.confusion_matrix(y_test, y_pred))

print("The predicted values are: \n", y_pred)
print("\nCrosstab:\n", pd.crosstab(y_test, y_pred, margins=True))

"""7.  Use regularization parameters of max_depth, min_sample_leaf to recreate the model. What is the impact on the model accuracy? How does regularization help? (20 points) """

max_depth_parameter = np.arange(1,21)
min_sample_leaf_parameter = np.arange(1,21)
from sklearn.metrics import accuracy_score
for k in max_depth_parameter:
    for l in min_sample_leaf_parameter:
        print("max_depth_parameter",k)
        model2 = DecisionTreeClassifier(criterion = 'entropy', max_depth = k, min_samples_leaf = l)
        model2.fit(X_train, y_train)
        Y_predict2 = model2.predict(X_test)
        print("min_sample_leaf_parameter",l,"The Accuracy Score:", accuracy_score(y_test, Y_predict2))
    print("\n")

#From the above analysis, max_depth value= 6 and min_sample_leaf value = 2 gives The Accuracy Score: 0.9152542372881356 which is better than the accuracy ie 0.864406779661017 achieved before.
#There is some impact on the accuracy of the model due to regularization of paameters. It has increased by increasing the max_depth factor.Regularization is done to prevent the overfitting of the data. It can make our model more accurate, which will lead to more accurate predictions overall.However, it may not always be worth our while as sometimes, there is no impact at all

#To avoid overfitting, we need to restrict the Decision Trees’ freedom during the tree creation. This is called regularization
#regularization is generally done to reduce the model complexity and  thereby brings down the variance  of the model. This helps overcome the overfitting problem.

# As individual trees are prone to overfitting, but this can be alleviated by regularizing the trees in random forest model.

"""8. Next implement the decision tree using Random Forest. What is the optimal number of trees that gives the best result? (10 points)"""

from sklearn.ensemble import RandomForestClassifier
acc_scores=[]
optimal_trees=np.arange(10,400)
for i in optimal_trees:
    model=RandomForestClassifier(n_jobs=2,n_estimators=i,criterion="entropy",random_state=143)
    model.fit(X_train,y_train)
    y_pred_rf=model.predict(X_test)
    rf_score=metrics.accuracy_score(y_test, y_pred_rf)
    acc_scores.append(rf_score)

acc_scores

max(acc_scores)

acc_scores.index(0.9322033898305084)

plt.plot(optimal_trees, acc_scores)
plt.xlabel('optimal_trees')
plt.ylabel('Accuracy')

# The optimal number of trees that gives the best result is 30 .(as the loop starts from 10 to 400 and index value achieved is 19 so 10+19+1(because index starts from 0))

